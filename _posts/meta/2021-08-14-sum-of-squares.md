---
title: "SpongeBob SquaredErrors"
subtitle: "The most popular error function for regression is here!"
background: '/img/learning/bg-math.png'
---

Second post!

The first chapter [has been published](/learning/math/sum-of-squares), and it's about the most widely used error function in regression: the sum of squared errors (or just "sum of squares").

I was already expecting this endeavour to be hefty work. Nothing could, however, prepare my inexperienced self to the hardships of trying to write content that is:
- Correct (duh)
- Exposed in a manner that, being kept as simple as possible, helps readers to gain some actual intuition about the subject
- Presented formally enough that readers can go to other sources (scientific articles, blogs, textbooks, etc.) and not feel held back by different notations or levels of formality

Though it took a bit of time, writing this content came with some bonuses:
- I had to brush up on my Matplotlib and LaTeX to generate some plots and equations, respectively
- The rigor required to publish something that is scientifically correct forced me to do a ton of research, during which I learned a lot and added about 10 new links to my "To Read" list
- It made me realize how little I understood about optimization, which rendered me to commit to writing content about it
- I also came to the realization that, even though many systems we are trying to optimize have closed-form solutions, we still use analytic methods to approximate them. More on that when the Optimization chapter comes out :)

For the short time I've been working on this, I've learned a lot. I'm getting right back to work and can't wait for my next post annoucing a new published chapter. Talk to you in about 4 months.

Cheers!