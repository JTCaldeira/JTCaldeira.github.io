---
title: "Drop The Gradient"
subtitle: "Meet gradient descent, the most popular optimization algorithm and the backbone of modern deep learning"
background: '/img/learning/bg-math.png'
---

I'm gonna stop counting posts now.

The next Prerequisites chapter [is live](/learning/prerequisites/gradient-descent)! Therein we take a look at the most popular optimization algorithm, which also happens to be the backbone of most modern deep learning techniques.

I jokingly said in the last post how I would only post again in 4 months, but it took a whole lot more time than I expected to get more content out. Turns out that working on a thesis proposal while taking cybersecurity subjects isn't trivial. Who knew.

In happier news, the soon-to-be-published chapter on Optimization is receiving its finishing touches. I think that for now, that will wrap up the Prerequisites section. Talk to you in... No compromises.

Cheers!
